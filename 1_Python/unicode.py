# -*- coding: utf-8 -*-
"""
Created on Wed May 24 14:49:56 2023

@author: Atharv
"""

#normalizing unicode text to standard representation
s1='Spicy Jalape\u00f1o'
s2='Spicy Jalapen\u0303o'
print(s1)
print(s2)
#here string are same but we use different encoding format so it is show False
s1==s2
################
#to remove previous drawback use normalization(making it in uniform pattern)
import unicodedata
t1=unicodedata.normalize('NFC',s1)
t2=unicodedata.normalize('NFC',s2)
t1==t2
#it will show the True because it will ignore the pattern
##########################
#normalization using NFD
t3=unicodedata.normalize('NFD',s1)
t4=unicodedata.normalize('NFD',s2)
t3==t4
####################
#ASCII generated by NFC and NFD are different
print(ascii(t1))
print(ascii(t3))
######################
t1=unicodedata.normalize('NFD', s1)
''.join(c for c in t1 if not unicodedata.combining(c))
#######################
string1='apple'
string2='jellie123'
string3='12345'
string4='pre@12'

string1.encode(encoding='utf-8',errors='strict')
string2.encode(encoding='utf-8',errors='strict')
string3.encode(encoding='utf-8',errors='strict')
string4.encode(encoding='utf-8',errors='string ')
#-->Out[9]: b'pre@12'
#In Python, when you see a string prefixed with a b like b'jellie123', 
#it indicates that the string is a bytes literal. 
#Bytes literals are used to represent sequences of bytes, 
#which are often used to store binary data or text data encoded in 
#a specific character encoding.

#################################################
text='one 🐘 and three 🐋'
e=text.encode(encoding='utf-8',errors='strict')
e

e=text.encode(encoding='utf-16',errors='strict')
e
##################################################
fname='data.txt'
with open(fname,mode='rb') as fo:
    content=fo.read()
    print(type(content))
    print(content)
    print(content.decode('utf-8'))
    
with open(fname,mode='rb') as fo:
    content=fo.read()
    print(type(content))
    print(content)
    print(content.decode('utf-16'))
#-->Errror
###################################################
#Working with unicode character in RE
#re package can be use to the  normalization
import re
num=re.compile('\d+')
num.match('123')
#######
pat=re.compile('stra\u00dfe',re.IGNORECASE)
s='straße'
pat.match(s)
#############
#Stripping unwanted characters from string
#1)Whitespace stripping
s='  Hello World '
s.strip()
##########
#remove LHS whitespace
s='  hello world'
s.lstrip()
#############
#remove RHS whitespace
s='Hello World  '
s.rstrip()
##############
#2)Character Stripping
t='-------hello======'
t.rstrip('=')
t.lstrip('-')
t.strip('-=')
########################
#It will remove the whitespaces 
s=' hello world       \n'
s.strip()
#######################
#' '-->whitespcae
#''--->No space
s.replace(' ','')
########################
s = 'pýtĥöñ\fis\tawesome\r\n'
s#It will show the ascii value for certain text
#to remove this use translate()
remap={
       ord('\t'):' ',
       ord('\f'):' ',
       ord('\r'):None#deleted
       }
a=s.translate(remap)
a
#############################
#to remove all combining character
import unicodedata
import sys
cmb_chrs=dict.fromkeys(c for c in range(sys.maxunicode) if unicodedata.combining(chr(c)))
b=unicodedata.normalize('NFD',a)
b
b.translate(cmb_chrs)
###############################
#Another method to clean up the text
a='pýtĥöñ is awesome\n'
b=unicodedata.normalize('NFD',a)
b.encode('ascii','ignore').decode('ascii')
########################
#Feature Engineering Technoiques
#Aligning the text string
text='Hello World'
text.rjust(20)
text.ljust(20)
text.center(20)
########################
#add some character in text aligning
text.rjust(20,'-')
text.ljust(20,'=')
text.center(20,'-')
######################3
format(text,'>20')#> specify direction(RHS)
format(text,'<20')#LHS
format(text,'^20')#center
########################
format(text,'=>20')
format(text,'=<20')
format(text,'#^20')
#########################
'{:>10s} {:>10s}'.format('Hello','World')
###########################3
x=1.2345
format(x, '>10')
format(x, '^10.2f')
#########################3
parts=['Is','Chicago','Not','Chicago?']
' '.join(parts)
','.join(parts)
''.join(parts)

##############################
#if you join very few string then you can use + operator
a='Is Chicago'
b='Not Chicago?'
c=a+' '+b
c
#################################
print('{} {}'.format(a,b))
#################################
print(a+' '+b)
################################
a='Hello' 'world'
a
#here we don't use any concatenation operator so it will directly concatenate
#the strings without any whitespace
#############################
#Interpolating variables in string
s='{name} has {n} message'
s.format(name='Atharv',n=21)
###############################
name='Atharv'
n=21
s.format_map(vars())
#############################
#Text wrapping
import textwrap
s='hello good morning ,I am atharv joshi from computer department .I am passionate about coding'
print(textwrap.fill(s,70))
print(textwrap.fill(s,30))
############################
#Initial Identation 
print(textwrap.fill(s,30,initial_indent='       '))
############################
#Subsequent lines indentation
print(textwrap.fill(s,30,subsequent_indent='       '))
###########################
#Tokenization
import nltk
nltk.download('punkt')#It is used for punctuation
sentence_data='I am Atharv Joshi. I am CSE student'
nltk_tokens=nltk.sent_tokenize(sentence_data)
print(nltk_tokens)
#########################
#Non english tokenizatioon

german_tokenizer=nltk.load('tokenizers/punkt/german.pickle')
german_tokens=german_tokenizer.tokenize('Wie geht es Ihnen? Gut , Danke')
print(german_tokens)
##############################
#word tokenizer
import nltk
word_data='I am atharv. Good morning'
nltk_tokens=nltk.word_tokenize(word_data)
print(nltk_tokens)
##################
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopwords.words('english')
###################
# Various langauage stopwords

from nltk.corpus import stopwords
print(stopwords.fileids())
###########################################

from nltk.corpus import stopwords
en_stops=set(stopwords.words('english'))

all_word=['There ','is','a','tree','near','the','river']
for word in all_word:
    if word not in en_stops:
        print(word)
##########################################

# Find synonyms words

import nltk
nltk.download('omw-1.4')
from nltk.corpus import wordnet

nltk.download('wordnet')

synonyms=[]

for syn in wordnet.synsets("Soil"):
    for lm in syn.lemmas():
        synonyms.append(lm.name())
print(set(synonyms))


# synonyms for tree
synonyms=[]
for syn in wordnet.synsets("tree"):
    for lm in syn.lemmas():
        synonyms.append(lm.name())
print(set(synonyms))

# Find Antonyms words

import nltk
nltk.download('omw-1.4')
from nltk.corpus import wordnet

nltk.download('wordnet')

antonyms=[]
for syn in wordnet.synsets("ahead"):
    for lm in syn.lemmas():
        if lm.antonyms():
            antonyms.append(lm.antonyms()[0].name())
            
print(set(antonyms))

































